version: "3.9"
services:
  namenode1:
    # image: hadoop
    build: ./hadoop
    hostname: namenode1
    ports:
      - 8088:8088
      - 8032:8032
      - 9870:9870
      - 50070:50070
      - 50075:50075
    volumes:
      - nn1-data:/dfs
      - nn1-conf:/opt/hadoop/etc/hadoop
    networks:
      hadoop:
        ipv4_address: 192.168.10.10
    extra_hosts:
      - "namenode2: 192.168.10.11"
      - "datanode: 192.168.10.12"
      - "thangdv: 192.168.10.1"
      - "sparknode: 192.168.10.20"
      - "zoo1: 192.168.10.31"
      - "zoo2: 192.168.10.32"
      - "zoo3: 192.168.10.33"

  namenode2:
    # image: hadoop
    build: ./hadoop
    hostname: namenode2
    volumes:
      - nn2-data:/dfs
      - nn2-conf:/opt/hadoop/etc/hadoop
    ports:
      - 9871:9870
    networks:
      hadoop:
        ipv4_address: 192.168.10.11
    extra_hosts:
      - "namenode1: 192.168.10.10"
      - "datanode: 192.168.10.12"
      - "thangdv: 192.168.10.1"
      - "sparknode: 192.168.10.20"
      - "zoo1: 192.168.10.31"
      - "zoo2: 192.168.10.32"
      - "zoo3: 192.168.10.33"

  datanode:
    # image: hadoop
    build: ./hadoop
    hostname: datanode
    volumes:
      - dn-data:/dfs
      - dn-conf:/opt/hadoop/etc/hadoop
    networks:
      hadoop:
        ipv4_address: 192.168.10.12
    extra_hosts:
      - "namenode1: 192.168.10.10"
      - "namenode2: 192.168.10.11"
      - "thangdv: 192.168.10.1"

  sparknode:
    # image: spark
    build: ./spark
    hostname: spark
    volumes:
      - sp-conf:/opt/spark/conf
    networks:
      hadoop:
        ipv4_address: 192.168.10.20
    extra_hosts:
      - "namenode1: 192.168.10.10"
      - "namenode2: 192.168.10.11"
      - "thangdv: 192.168.10.1"

  zoo1:
    image: zookeeper
    hostname: zoo1
    ports:
      - 2181:2181
    environment:
      ZOO_MY_ID: 1
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181
    networks:
      hadoop:
        ipv4_address: 192.168.10.31
    extra_hosts:
      - "namenode1: 192.168.10.10"
      - "namenode2: 192.168.10.11"
      - "zoo2: 192.168.10.32"
      - "zoo3: 192.168.10.33"

  zoo2:
    image: zookeeper
    hostname: zoo2
    ports:
      - 2182:2181
    environment:
      ZOO_MY_ID: 2
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181
    networks:
      hadoop:
        ipv4_address: 192.168.10.32
    extra_hosts:
      - "namenode1: 192.168.10.10"
      - "namenode2: 192.168.10.11"
      - "zoo1: 192.168.10.31"
      - "zoo3: 192.168.10.33"

  zoo3:
    image: zookeeper
    hostname: zoo3
    ports:
      - 2183:2181
    environment:
      ZOO_MY_ID: 3
      ZOO_SERVERS: server.1=zoo1:2888:3888;2181 server.2=zoo2:2888:3888;2181 server.3=zoo3:2888:3888;2181
    networks:
      hadoop:
        ipv4_address: 192.168.10.33
    extra_hosts:
      - "namenode1: 192.168.10.10"
      - "namenode2: 192.168.10.11"
      - "zoo1: 192.168.10.31"
      - "zoo2: 192.168.10.32"

networks:
  hadoop:
    driver: bridge
    ipam:
      config:
        - subnet: 192.168.10.0/16
          gateway: 192.168.10.1

volumes:
  nn1-data:
    driver: local
    driver_opts:
      type: bind
      device: ./namenode1/data
      o: bind
  nn1-conf:
    driver: local
    driver_opts:
      type: bind
      device: ./namenode1/conf
      o: bind
  nn2-data:
    driver: local
    driver_opts:
      type: bind
      device: ./namenode2/data
      o: bind
  nn2-conf:
    driver: local
    driver_opts:
      type: bind
      device: ./namenode2/conf
      o: bind
  dn-data:
    driver: local
    driver_opts:
      type: bind
      device: ./datanode/data
      o: bind
  dn-conf:
    driver: local
    driver_opts:
      type: bind
      device: ./datanode/conf
      o: bind
  sp-conf:
    driver: local
    driver_opts:
      type: bind
      device: ./sparknode/conf
      o: bind
